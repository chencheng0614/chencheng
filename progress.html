<!DOCTYPE html>
    
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>Chris Chen | Communication Designer of Future Media </title>
		<link rel="stylesheet" href="CSS/research.css">

		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">

		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
	</head>


	<style>
	

	* {font-family: 'Lato', sans-serif;}
	

	div.socialmedia{
		background-color: #2c3e50;
		text-align: center;
		margin-top:2500px;
		margin-bottom: 50px;
}

	</style>

<body style="padding-bottom:10px;">
	    <header>
		
			<img src="images/CC-logos.jpeg" alt="CV Logo" width="50">
			<h2>Chris Chen </h2>
			<h3>Communication Designer of Future Media</h3>
	
			<div class="absolute">
			<nav id= "nav_menu">
				<ul>
					<li><a href = "index.html" target="_parent">Home</a></li>	
					<li><a href = "research.html" target="_parent">Professional</a></li>	
					<li><a href = "personal.html" target="_parent">Personal</a></li>	
				</ul>
			
		</nav>
	</div>

</header>

	<main>
	
		<h4 class="pub"> Could this be Racially Biased? Displaying Training Data Diversity and Soliciting Feedback to Help Users Calibrate Trust in AI</h4>
		<h6 class="authors"> Cheng Chen & S. Shyam Sundar </h6> 
		<p class="abstract">Training data diversity is important for ensuring group fairness 
			in decisions made by AI algorithms. 
			This study explores if displaying racial diversity in training data and 
			labelers’ backgrounds on the interface will shape user trust in AI, 
			and whether that will break down if the system shows racial bias in its 
			performance. In a 2 (racial diversity in training data: presence vs. 
			absence) × 2 (racial diversity in labelers’ backgrounds: presence vs. 
			absence) × 2 (AI performance: racially biased vs. unbiased) × 2 
			(user feedback: yes vs. no) experiment, we discovered that the 
			presence of racial diversity in training data and labelers’ 
			backgrounds triggers the representativeness heuristic, which 
			leads to higher expectations of AI fairness and accuracy as well as 
			trust in AI. However, inviting user feedback lowers behavioral trust 
			when there is no obvious bias in AI performance. Implications for 
			communicating training data quality via explainable interfaces are discussed. </p>
		<hr>

		<h4 class="pub"> Is this AI trained on Credible Data? The Effects of Labeling Quality and Performance Bias on User Trust
			</h4>
		<h6 class="authors"> Cheng Chen & S. Shyam Sundar </h6> 
		<p class="abstract">To promote data transparency, frameworks such as CrowdWorkSheets 
			encourage   documentation of annotation practices on the interfaces of AI systems,
			 but we do not know how they affect user experience. 
			 Will the quality of labeling affect perceived credibility of training data? 
			 Does the source of annotation matter?  
			 Will a credible dataset persuade users to trust a system even if 
			 it shows racial biases in its predictions? We conducted a user study (N = 430) 
			 with a prototype of a classification system, using a 2 
			 (labeling quality: high vs. low) × 4 (source: others-as-source vs. 
			 self-as-source cue vs. self-as-source voluntary action, vs. 
			 self-as-source forced action  ) × 3 (AI performance: none vs. biased 
			 vs. unbiased) experiment. We found that high-quality labeling leads 
			 to higher perceived training data credibility, which in turn enhances
			  users’ trust in AI, but not when the system shows bias. Practical 
			  implications for explainable and ethical AI interfaces are discussed. </p>
		<hr>

		<h4 class="pub"> What do Users Get Out of AI Technologies? 
			Construction and Validation of an AI Gratifications Scale
		</h4>
		<h6 class="authors"> Cheng Chen, Carlina DiRusso, Ruosi Shao, Hyun Yang, Michael Krieger, & S. Shyam Sundar </h6> 
		<p class="abstract">This study explores why people use AI-based tools, 
			with a focus on identifying the major gratifications obtained by them. 
			While prior studies have identified specific gratifications obtained 
			by users of specific AI tools, the literature is yet to identify a 
			universal set of gratifications that is common across most AI interfaces. 
			As HCI designers increasingly treat AI as a distinct technology, 
			it is important to identify a common set of factors that shape user 
			experience of AI. With this goal, and guided by the Uses and Gratifications 
			(U&G) framework, we conducted two survey studies (combined N = 1,264), 
			which resulted in a reliable and valid 29-item AI Gratifications Scale, 
			that loaded cleanly onto seven factors: anthropomorphism, privacy 
			consciousness, informality, seamlessness, accuracy, fairness, and 
			acknowledgment of limitations. This 7-factor gratifications 
			scale was invariant across nine major AI functions. We discuss 
			practical implications of these findings for designing and assessing
			 human-AI interaction.  </p>
	<hr>

	</main>	

    <aside>
        <nav id ="nav_research">
            <ul>
                <li><a href = "publication.html">Publications</a></li>
                <li><a href = "conference.html">Presentations</a></li>
                <li><a href = "progress.html">In Progress</a></li>
            </ul>
    </nav>

    </aside>
  

    <footer>
		<div class="socialmedia-progress">
			<a href="https://scholar.google.com/citations?user=03k_ATMAAAAJ&hl=en" target="_blank">
				<img src="images/Google_Scholar_logo.svg.png" width="40px"></a>
			<a href="https://www.researchgate.net/profile/Cheng-Chen-53?ev=hdr_xprf&_sg=PV0Fjah1FwscCcvIzMlS9NvV4bheELzs05jedKAFneMMKrAdRLnrk5fBbMFez7-trXmkBwcIn4ctWOBuMx5Z0M5V" target="_blank">
				<img src="images/research gate logo.png" width="40px"></a>
			<a href="https://www.facebook.com/cheng.chen.5680" target="_blank">
				<img src="images/Facebook_Logo_(2019).png" width="40px"></a>
		</div>	

		<div class="contact"> 
			<p> School of Communications, 
				Elon University, 100 Campus Drive, Elon, NC 27244</p>
			<p> Email: cchen8@elon.edu</p>
		</div>	
		

	</footer>
</body>
    </html>
